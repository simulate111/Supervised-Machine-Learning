{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Supervised-Machine-Learning/blob/main/quiz3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u37vDsq1bSj7"
      },
      "source": [
        "# Quiz 3 - Programming Excercises\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "In this exercise, you will implement a Logistic Regression model and explore different stepsize policies for the Stochastic Gradient Descent algorithm using the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html).\n",
        "\n",
        "The code below prepares the data required for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YcUPq4CCbSkE",
        "outputId": "86be6887-e336-4b18-8475-f7adb4a7ebe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input data: (569, 30)\n",
            "Shape of output data: (569,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# load the data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "print(f\"Shape of input data: {X.shape}\")\n",
        "print(f\"Shape of output data: {y.shape}\")\n",
        "\n",
        "mdata, ndim = X.shape\n",
        "\n",
        "# convert the output space from {0,1} into {-1,+1}\n",
        "y = 2*y - 1\n",
        "\n",
        "# normalization\n",
        "X /= np.outer(np.ones(mdata), np.max(np.abs(X),0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmaXRmRlbSkJ"
      },
      "source": [
        "**Task 1**: Complete the `logreg_sgd_cls` class provided below to implement a Logistic Regression class.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "<b>Note:</b> Be aware that the Logistic Regression algorithm presented in Lecture 5 may differ from the one implemented in Sklearn. Please use the version presented in the Lecture.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T81NwsBubSkL"
      },
      "outputs": [],
      "source": [
        "class logreg_sgd_cls:\n",
        "    \"\"\"\n",
        "    Logistic Regression Classifier\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.nitermax = 100     # maximum iteration\n",
        "        self.eta = 0.1          # initial step size\n",
        "        self.w = None           # weights to learn\n",
        "\n",
        "    ## ------------------------------------------\n",
        "    def fit(self, X, y, eta, nitermax, diminish=0):\n",
        "        \"\"\" Train the logistic regression model\n",
        "            with stochastic gradient descent algorithm\n",
        "\n",
        "        Input:  X         2D array where each row represents an input example\n",
        "                y         1D array(vector) of +1,-1 labels\n",
        "                eta       initial step size\n",
        "                nitermax  maximum number of iterations\n",
        "                diminish  =0 constant step size, =1 diminishing step size\n",
        "        \"\"\"\n",
        "        mtrain, ndim = X.shape\n",
        "\n",
        "        # initialize the weights\n",
        "        w = np.zeros(ndim)\n",
        "\n",
        "        # iterations on the full data\n",
        "        for t in range(nitermax):\n",
        "\n",
        "            # select the stepsize\n",
        "            if diminish == 1:       # diminishing stepsize policy\n",
        "                etat = eta/(t+1)    # t+1 to avoid division by zero\n",
        "            else:                   # constant stepsize policy\n",
        "                etat = eta\n",
        "\n",
        "            # perform one step of stochastic gradient descent on each training example\n",
        "            for i in range(mtrain):\n",
        "                # TODO: implement the weight update\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # save the final weights to the model\n",
        "        self.w  = w\n",
        "\n",
        "        return(w)\n",
        "\n",
        "    ## ------------------------------------------\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict the labels\n",
        "\n",
        "        Input:\n",
        "            X   2D array where each row represents an input example\n",
        "\n",
        "        Output:\n",
        "            y   1D array of predicted labels\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: implement the prediction process X, self.w -> y\n",
        "        y = None\n",
        "\n",
        "        return(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyBwZk99bSkO"
      },
      "source": [
        "**Task 2**: Complete the helper function below for running n-Fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4PW7R3ffbSkP",
        "outputId": "b0cd1594-bf5f-4cb3-bca6-568275510784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 27 (<ipython-input-3-945509521c97>, line 37)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-945509521c97>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    return(xf1score)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 27\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def learning_cycle(X, y, niteration, eta, stepsize_type, nfold):\n",
        "    \"\"\"\n",
        "    Helper function for running n-Fold cross validation\n",
        "    Input:  X              2d array of inputs\n",
        "            y              1d array of outputs\n",
        "            niteration     number of iteration for gradient descent\n",
        "            eta            initial stepsize\n",
        "            stepsize_type  =0 constant, =1 diminishing\n",
        "            nfold          number of folds for cross-validation\n",
        "    Output: xf1score       1D array of size [nfold] containing the f1 scores for each fold\n",
        "    \"\"\"\n",
        "\n",
        "    # split the data into n folds\n",
        "    cselection = KFold(n_splits=nfold, random_state=None, shuffle=False)\n",
        "\n",
        "    # array to collect the results for each fold\n",
        "    xf1score = np.zeros(nfold)\n",
        "\n",
        "    # create an instance of the logistic regression model\n",
        "    clogreg = logreg_sgd_cls()\n",
        "\n",
        "\n",
        "    # perform n-fold cross-validation\n",
        "    for ifold, (index_train, index_test) in enumerate(cselection.split(X)):\n",
        "        # TODO: train the model, and evaluate using F1 score for each fold\n",
        "\n",
        "        # xf1score[ifold] = ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return(xf1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Cnd3zhbSkS"
      },
      "source": [
        "**Task 3**: Execute n-fold cross-validation and answer question 4 according to the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-hxapHAbSkT"
      },
      "outputs": [],
      "source": [
        "# Learning hyperparameters\n",
        "# create the list of initial step sizes (learning rates)\n",
        "neta = 40   # number of different step size\n",
        "eta0 = 0.2\n",
        "# list of initial step sizes\n",
        "leta = [ eta0*(i+1) for i in range(neta)]\n",
        "\n",
        "# number of iterations in gradient descent\n",
        "iteration = 50\n",
        "\n",
        "# number of folds\n",
        "nfold = 5\n",
        "\n",
        "# fix the random seed\n",
        "rng = np.random.default_rng(12345)\n",
        "\n",
        "# number of different stepsize policies\n",
        "nstepsize_type = 2\n",
        "\n",
        "# array to collect the test scores for the two stepsize policies\n",
        "xmean_test_results = np.zeros((neta, nstepsize_type))\n",
        "\n",
        "\n",
        "# Run cross-validation\n",
        "# enumerate stepsize policies\n",
        "for istepsize in [0, 1]:    # 0 - constant, 1 - diminishing\n",
        "\n",
        "    xf1score_eta = np.zeros(neta)\n",
        "\n",
        "    # enumerate the (initial) stepsizes\n",
        "    for ieta in range(neta):\n",
        "        eta = leta[ieta]\n",
        "\n",
        "        # n-fold cross-validation\n",
        "        xf1score = learning_cycle(X, y, iteration, eta, istepsize, nfold)\n",
        "        # compute the mean of the test accuracies\n",
        "        xf1score_eta[ieta] = np.mean(xf1score)\n",
        "\n",
        "    xmean_test_results[:, istepsize] = xf1score_eta\n",
        "\n",
        "# values of the curves in the graph\n",
        "for i in range(nstepsize_type):\n",
        "    for j in range(neta):\n",
        "        print('%6.4f'%leta[j],'%6.4f'%xmean_test_results[j, i])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}